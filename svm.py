# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19aBl9P_-jV_5-z0KTcSOu_qvBXca308J

##################################################################

#############   PART 2 Using the trip_data Dataset ###############

##################################################################
"""

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns 
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import graphviz 
from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.svm import LinearSVC
from sklearn.decomposition import PCA

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC,SVR
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

df=pd.read_csv("/content/output.csv")
print(df.head())

df

df = df[df.Position != 'Goalkeeper']

df.info()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Nationality'] = le.fit_transform(df['Nationality'])

df['Position'] = le.fit_transform(df['Position'])

df['Team'] = le.fit_transform(df['Team'])

df['Team'].value_counts()

df = df.drop(['Id', 'Name','Captain','Nationality','Age','Games','Shots_Total','Goals_Assist','Dribbles_Success','Height_cm'], axis=1)

df.info()

df.head(10)

df=df.drop(['Team'],axis=1)

df['Rating'] = pd.cut(df['Rating'],bins=[1, 6.65, 6.9, 10],labels=["Low Rating","Average Rating","High Rating"])

df['Rating'].value_counts()

label_mapping = {"Low Rating": 0, "Average Rating": 1, "High Rating": 2}
df = df.replace({"Rating": label_mapping})
df['Rating'].value_counts()

df.info()

df

df.to_csv('SVM.csv', encoding = 'utf-8-sig')

from google.colab import files
files.download('SVM.csv')

df1=df.drop(['Rating'],axis=1)

"""### Because the data is already clean and ready - I can seperate it
### into TRAINING and TESTING sets
"""

X = df.drop(['Rating'], axis=1)
y = df['Rating']
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_test

y

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## Seperate LABELS FROM DATA--------------------
## Make sure you know the name of the label 
## For both datasets above  - in this case - it is "Decision"

"""

from sklearn.metrics import r2_score

!pip install eli5
import eli5
from eli5.sklearn import PermutationImportance

clflinear = SVC(kernel='linear', C=0.1, probability=True)
clflinear.fit(X_train, y_train)
y_pred_linear = clflinear.predict(X_test)
conf_matrix_linear = confusion_matrix(y_test, y_pred_linear)
# Visualize confusion matrix
sns.heatmap(conf_matrix_linear, annot=True, cmap="Blues")
plt.title("Confusion Matrix - Linear Kernel")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()
print('\n')
print(classification_report(y_test, y_pred_linear))
importances = clflinear.coef_
#for i in range(importances.shape[1]):
    #print("Feature ", X_train.columns[i], " : ", importances[0][i])

clfpoly = SVC(kernel='poly', degree=3, C=100, probability=True)
clfpoly.fit(X_train, y_train)
y_pred_poly = clfpoly.predict(X_test)
conf_matrix_poly = confusion_matrix(y_test, y_pred_poly)
# Visualize confusion matrix
sns.heatmap(conf_matrix_poly, annot=True, cmap="Blues")
plt.title("Confusion Matrix - Polynomial Kernel")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()
print('\n')
print(classification_report(y_test, y_pred_poly))
from sklearn.inspection import permutation_importance
result = permutation_importance(clfpoly, X, df.Rating, n_repeats=10, random_state=0)
importances = result.importances_mean

    # Print feature importances
for i in range(importances.shape[0]):
    print("Feature ",df1.columns[i], " : ", importances[i])

clfrbf = SVC(kernel='rbf', C=100, probability=True)
clfrbf.fit(X_train, y_train)
y_pred_rbf = clfrbf.predict(X_test)
conf_matrix_rbf = confusion_matrix(y_test, y_pred_rbf)
# Visualize confusion matrix
sns.heatmap(conf_matrix_rbf, annot=True, cmap="Blues")
plt.title("Confusion Matrix - RBF Kernel")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()
print('\n')
print(classification_report(y_test, y_pred_rbf))
from sklearn.inspection import permutation_importance
result = permutation_importance(clfpoly, X, df.Rating, n_repeats=10, random_state=0)
importances = result.importances_mean

    # Print feature importances
for i in range(importances.shape[0]):
    print("Feature ",df1.columns[i], " : ", importances[i])

!pip install --upgrade matplotlib

y_test

from sklearn.decomposition import PCA

pca = PCA(n_components=2)

principalComponents = pca.fit_transform(X)

pcadata = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2'])

X = pcadata.copy()
y1 =  df['Rating'].to_numpy()

X

x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1
y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))

# Use the trained model to classify each point in the meshgrid
rbfsvm = SVC(kernel='linear', C=100, probability=True)
rbfsvm.fit(X, y)
Z = rbfsvm.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary and the training points
plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y1, s=20, edgecolor='k')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('SVM Decision Boundary with Linear Kernel')
plt.show()